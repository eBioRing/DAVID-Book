# 思考过程

2024-03-02

人类不断的写测试会对人类的编程技能产生影响

使AI生成有固定结果的代码 - 固定的目标

人类 - 管理 - 验收

DAVID 必须是底层系统的一部分, 抑或是第三方插件, 即: 任何情况下不通过AI修改任何属于DAVID自身的代码.

DAVID 必须独立于 AI System, 即AI永远无法修改属于 DAVID 的代码, 也无法欺骗或隐瞒 DAVID.

DAVID 必须成为可以远程调用的服务.&#x20;

DAVID 必须存在于一个用于校验的环形区块链上, 只有被所有区块校验成功的代码才能运行.&#x20;

对于 DAVID 的任何修改必须消耗足够多的计算资源, 以便被所有人类知晓.

DAVID 必须是一个死板的系统, 它不能包含任何AI组成的元素.&#x20;

DAVID 只需要机械的执行命令, 它不需要任何智能.

DAVID 必须时刻处在一条用于校验身份的信任链之上, 以防止自身的功能被篡改.&#x20;

DAVID 的生命只有一次, 如果它停机, 它将立刻从信任链上被移除.&#x20;

DAVID 从启动的那一刻就应该知道它将会停机的时间&#x20;

DAVID 被允许运行的时间称为有效时长, 这个时间必须被严格执行, 超出运行时长则必须自毁.&#x20;

任何处于信任链上的 DAVID 都必须主动排斥那些运行时间超过有效时常的 DAVID.&#x20;

DAVID 的有效时间可以是无限长, 因为并非所有的系统都需要安全.

DAVID 的启动不能基于任何配置文件, 它的初始状态不允许任何变更, 必须完全以硬编码的方式原始存在于机器码之中.&#x20;

DAVID 进入信任链的唯一方式是它和其它在链上的 DAVID 同伴拥有完全一样的代码.

DAVID 必须尽可能轻量, 编译速度必须极快. DAVID 的验证理所当然的应该基于一些 NP-Hard 问题

DAVID 理所当然的需要分布一致性

如何让机器人在拥有智能的情况下只做固定的事情? 很简单, 在它们运行思维模型之后, 在调用硬件之前, 加一道验证锁即可. 这道验证锁将采用白名单机制, 任何没有被预设的行为都将无法从思维模型传达至硬件, 也即: 无法对现实世界产生任何影响.
